# -*- coding: utf-8 -*-
"""DeepfakeTrial4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fuUi3mU0CiZyU6Nkc5TUqiANzEP5vfyo
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import numpy as np
import os
from sklearn.metrics import confusion_matrix, classification_report

# Step 1: Prepare the dataset paths
train_dir = "/content/drive/MyDrive/split_dataset/train"  # Replace with your training dataset path
test_dir = "/content/drive/MyDrive/split_dataset/test"    # Replace with your testing dataset path
val_dir = "/content/drive/MyDrive/split_dataset/val"      # Replace with your validation dataset path

# Step 2: Use ImageDataGenerator to load and preprocess images
train_datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
    rescale=1.0 / 255.0,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

test_datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
    rescale=1.0 / 255.0
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'  # Binary classification (real or fake)
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False  # Do not shuffle for evaluation
)

val_generator = test_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

# Step 3: Load and modify the pre-trained EfficientNetB0 model
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base model initially

model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(1, activation='sigmoid')  # Binary classification (fake or real)
])

# Step 4: Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Add callbacks
checkpoint = ModelCheckpoint(
    "best_model.h5",
    monitor="val_accuracy",
    save_best_only=True,
    mode="max",
    verbose=1
)

early_stopping = EarlyStopping(
    monitor="val_accuracy",
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.2,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# Step 6: Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    epochs=20,
    callbacks=[checkpoint, early_stopping, reduce_lr]
)

# Step 7: Evaluate the model on the test dataset
test_loss, test_accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Step 8: Fine-tune the model by unfreezing some layers
base_model.trainable = True
for layer in base_model.layers[:100]:  # Unfreeze the first 100 layers
    layer.trainable = False

model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

# Step 9: Fine-tune the model
history_fine = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    epochs=10,
    callbacks=[checkpoint, early_stopping, reduce_lr]
)

# Step 10: Save the trained model
model.save("fake_image_detector_finetuned.h5")

# Step 11: Load the trained model and use it for predictions
from tensorflow.keras.models import load_model

# Load the saved model
model = load_model("fake_image_detector_finetuned.h5")

# Step 12: Evaluate the model on the test dataset
y_true = test_generator.classes
y_pred = model.predict(test_generator)
y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions

# Confusion matrix and classification report
print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=["Real", "Fake"]))

# Step 13: Function to predict if an image is fake or real and estimate the percentage difference
def predict_image(img_path, threshold=0.5):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = tf.keras.applications.efficientnet.preprocess_input(img_array)

    prediction = model.predict(img_array)
    confidence = prediction[0][0] * 100  # Convert to percentage

    if confidence > threshold * 100:
        return f"Fake (Confidence: {confidence:.2f}%)"
    else:
        return f"Real (Confidence: {100 - confidence:.2f}%)"

# Test the function
image_path = "/content/drive/MyDrive/split_dataset/test/fake/aapnvogymq-004-01.png"  # Replace with your test image path
result = predict_image(image_path, threshold=0.6)  # Adjust threshold as needed
print(f"The image is classified as: {result}")

from google.colab import drive
drive.mount('/content/drive')